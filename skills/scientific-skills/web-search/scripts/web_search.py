#!/usr/bin/env python3
"""
Web Search â€” é€šç”¨ç½‘ç»œæœç´¢å·¥å…·

é€šè¿‡ Jina AI (r.jina.ai) + Google æœç´¢å®ç°é›¶é…ç½® web searchã€‚
æ— éœ€ API keyï¼Œä»…ä¾èµ– Python æ ‡å‡†åº“ã€‚

ç”¨æ³•:
    python web_search.py "æœç´¢å…³é”®è¯"                  # åŸºæœ¬æœç´¢
    python web_search.py "æœç´¢å…³é”®è¯" --num 10         # æŒ‡å®šç»“æœæ•°
    python web_search.py "æœç´¢å…³é”®è¯" --lang en        # æŒ‡å®šè¯­è¨€
    python web_search.py read "https://example.com"    # è¯»å–ç½‘é¡µå†…å®¹
"""

import sys
import urllib.request
import urllib.parse
import urllib.error
import argparse
import re


JINA_READER_BASE = "https://r.jina.ai/"
GOOGLE_SEARCH_BASE = "https://www.google.com/search"
DEFAULT_NUM = 5
DEFAULT_LANG = "zh-CN"
REQUEST_TIMEOUT = 30


def fetch_url(url: str) -> str:
    """é€šè¿‡ Jina AI Reader æŠ“å– URL å¹¶è¿”å› Markdown å†…å®¹ã€‚"""
    jina_url = JINA_READER_BASE + url
    req = urllib.request.Request(
        jina_url,
        headers={
            "User-Agent": "Mozilla/5.0 (compatible; WebSearchSkill/1.0)",
            "Accept": "text/plain",
        },
    )
    try:
        with urllib.request.urlopen(req, timeout=REQUEST_TIMEOUT) as resp:
            return resp.read().decode("utf-8", errors="replace")
    except urllib.error.HTTPError as e:
        return f"[é”™è¯¯] HTTP {e.code}: {e.reason}\nURL: {url}"
    except urllib.error.URLError as e:
        return f"[é”™è¯¯] æ— æ³•è¿æ¥: {e.reason}\nURL: {url}"
    except Exception as e:
        return f"[é”™è¯¯] {type(e).__name__}: {e}\nURL: {url}"


def build_search_url(query: str, num: int = DEFAULT_NUM, lang: str = DEFAULT_LANG) -> str:
    """æ„é€  Google æœç´¢ URLã€‚"""
    params = urllib.parse.urlencode({
        "q": query,
        "num": num,
        "hl": lang,
    })
    return f"{GOOGLE_SEARCH_BASE}?{params}"


def extract_search_section(raw: str) -> str:
    """ä»åŸå§‹ Markdown ä¸­æˆªå–æœç´¢ç»“æœåŒºåŸŸï¼ˆä»‹äºæ ‡é¢˜è¡Œå’Œé¡µè„šä¹‹é—´ï¼‰ã€‚"""
    # æœç´¢ç»“æœåŒºåŸŸèµ·å§‹æ ‡è®°ï¼ˆä¸­/è‹±æ–‡ï¼‰
    start_markers = ["æœç´¢ç»“æœ\n====", "Search Results\n===="]
    # æœç´¢ç»“æœåŒºåŸŸç»“æŸæ ‡è®°
    end_markers = ["ç½‘é¡µå¯¼èˆª\n====", "Page Navigation\n====",
                   "People also search for", "ç›¸å…³æœç´¢"]

    text = raw
    # æˆªå–èµ·å§‹
    for m in start_markers:
        idx = text.find(m)
        if idx != -1:
            text = text[idx + len(m):]
            break

    # æˆªå–ç»“æŸ
    best = len(text)
    for m in end_markers:
        idx = text.find(m)
        if idx != -1 and idx < best:
            best = idx
    text = text[:best]

    return text.strip()


def parse_results(section: str) -> list[dict]:
    """
    ä»æœç´¢ç»“æœåŒºåŸŸè§£æå‡ºç»“æ„åŒ–æ¡ç›®ã€‚

    æ¯æ¡ Google æœç´¢ç»“æœåœ¨ Jina Markdown ä¸­çš„å…¸å‹æ ¼å¼ï¼š
        [### Title ![Image N](...) Source https://...](actual_url)
        Source
        https://... â€º ...
        Date â€” Snippet text...
    """
    results = []

    # åŒ¹é…æ¯æ¡ç»“æœçš„å…¥å£ï¼š[### Title ... ](URL)
    # æ ‡é¢˜å†…å« ![Image](...) å’Œæ¥æºåŸŸåï¼Œéœ€è¦æ¸…ç†
    entry_pattern = re.compile(
        r'\[###\s+'        # å¼€å¤´ [###
        r'(.+?)'           # æ ‡é¢˜ï¼ˆè´ªå©ªæœ€çŸ­åŒ¹é…ï¼‰
        r'\]\('            # ](
        r'(https?://[^\)]+)'  # URL
        r'\)',              # )
    )

    matches = list(entry_pattern.finditer(section))
    if not matches:
        return results

    for i, m in enumerate(matches):
        raw_title = m.group(1)
        url = m.group(2)

        # æ¸…ç†æ ‡é¢˜ï¼šå»æ‰ ![Image N](...) å’Œå°¾éƒ¨çš„åŸŸå/è·¯å¾„é¢åŒ…å±‘
        title = re.sub(r'!\[Image[^\]]*\]\([^\)]*\)', '', raw_title).strip()
        # å»æ‰å°¾éƒ¨æ®‹ç•™çš„ URL é¢åŒ…å±‘ï¼ˆå¦‚ "https://zhuanlan.zhihu.com â€º ..."ï¼‰
        title = re.sub(r'\s*https?://\S+.*$', '', title).strip()
        # å»æ‰å°¾éƒ¨æ®‹ç•™çš„åŸŸåé¢åŒ…å±‘ï¼ˆå¦‚ "çŸ¥ä¹ä¸“æ  â€º ..."ã€"OpenAI Help Center â€º articles â€º"ï¼‰
        title = re.sub(r'\s+\S+\s+â€º.*$', '', title).strip()
        # å»æ‰çº¯ç²¹çš„ blob: å¼•ç”¨æ®‹ä½™
        title = re.sub(r'\s*blob:\S+', '', title).strip()

        # æå–æ‘˜è¦ï¼šä»å½“å‰åŒ¹é…ç»“æŸåˆ°ä¸‹ä¸€ä¸ªåŒ¹é…å¼€å§‹ä¹‹é—´çš„æ–‡æœ¬
        snippet_start = m.end()
        snippet_end = matches[i + 1].start() if i + 1 < len(matches) else len(section)
        between = section[snippet_start:snippet_end].strip()

        # ä»ä¸­é—´æ–‡æœ¬æå–æ‘˜è¦
        snippet_lines = []
        for line in between.split('\n'):
            line = line.strip()
            if not line:
                continue
            # è·³è¿‡çº¯ URL é¢åŒ…å±‘è¡Œï¼ˆå¦‚ "https://... â€º ..."ï¼‰
            if re.match(r'^https?://\S+', line):
                continue
            # è·³è¿‡çº¯æ¥æºåè¡Œï¼ˆé€šå¸¸å¾ˆçŸ­ä¸”ä¸å«æ ‡ç‚¹ï¼‰
            if len(line) < 15 and 'â€”' not in line and '...' not in line and 'ã€‚' not in line:
                continue
            # è·³è¿‡å­é“¾æ¥åˆ—è¡¨ï¼ˆå¦‚ "* [GPT-5](url)"ï¼‰
            if line.startswith('*'):
                continue
            # è·³è¿‡å†…åµŒçš„è§†é¢‘/å›¾ç‰‡å¡ç‰‡é“¾æ¥ï¼ˆå¦‚ "[OpenAI's New GPT ... YouTube Â· ..."ï¼‰
            if re.match(r'^\[.*(?:YouTube|è§†é¢‘).*\]\(', line):
                continue
            # è·³è¿‡ [Read more] é“¾æ¥
            line = re.sub(r'\[Read more\]\([^\)]*\)', '', line).strip()
            # å»æ‰ Markdown å¼ºè°ƒæ ‡è®°
            line = line.replace('_', '')
            # å»æ‰å†…åµŒçš„å›¾ç‰‡å¼•ç”¨ ![Image N]  æˆ– !Image N
            line = re.sub(r'!\[Image\s*\d*\](\([^\)]*\))?', '', line)
            line = re.sub(r'!Image\s*\d+', '', line)
            # å»æ‰å†…åµŒ markdown é“¾æ¥ï¼Œåªä¿ç•™æ–‡æœ¬
            line = re.sub(r'\[([^\]]*)\]\([^\)]*\)', r'\1', line).strip()
            # å»æ‰è¿ç»­å¤šä½™ç©ºæ ¼
            line = re.sub(r'\s{2,}', ' ', line).strip()
            if line:
                snippet_lines.append(line)

        # å»æ‰æ‘˜è¦å¼€å¤´å¯èƒ½æ®‹ç•™çš„æ¥æºåï¼ˆä¸æ ‡é¢˜æœ«å°¾é‡å¤çš„çŸ­æ–‡æœ¬ï¼‰
        if snippet_lines and len(snippet_lines[0]) < 30:
            first = snippet_lines[0]
            # æ¥æºåç‰¹å¾ï¼šçŸ­ã€æ— æ—¥æœŸã€æ— æ ‡ç‚¹
            has_date = bool(re.search(r'\d{4}', first))
            has_punct = bool(re.search(r'[ã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿ.,:;!?â€”]', first))
            if not has_date and not has_punct:
                snippet_lines = snippet_lines[1:]

        snippet = ' '.join(snippet_lines).strip()
        # æˆªæ–­è¿‡é•¿çš„æ‘˜è¦
        if len(snippet) > 300:
            snippet = snippet[:297] + '...'

        results.append({
            'title': title,
            'url': url,
            'snippet': snippet,
        })

    return results


def format_results(results: list[dict], query: str) -> str:
    """å°†ç»“æ„åŒ–ç»“æœæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„è¾“å‡ºã€‚"""
    if not results:
        return f"æœªæ‰¾åˆ°ä¸ \"{query}\" ç›¸å…³çš„æœç´¢ç»“æœã€‚è¯·å°è¯•å…¶ä»–å…³é”®è¯ã€‚"

    lines = []
    for i, r in enumerate(results, 1):
        lines.append(f"[{i}] {r['title']}")
        lines.append(f"    é“¾æ¥: {r['url']}")
        if r['snippet']:
            lines.append(f"    æ‘˜è¦: {r['snippet']}")
        lines.append("")

    lines.append(f"å…± {len(results)} æ¡ç»“æœã€‚")
    lines.append("æç¤º: ä½¿ç”¨ `python web_search.py read <URL>` å¯æŸ¥çœ‹æŸæ¡ç»“æœçš„å®Œæ•´ç½‘é¡µå†…å®¹ã€‚")
    return '\n'.join(lines)


def clean_search_results(raw: str, query: str) -> str:
    """ä»åŸå§‹ Jina Markdown ä¸­æå–å¹¶æ ¼å¼åŒ–æœç´¢ç»“æœã€‚"""
    section = extract_search_section(raw)
    if not section:
        # å›é€€ï¼šè¿”å›å»é™¤æ˜æ˜¾å™ªéŸ³åçš„åŸæ–‡
        return raw

    results = parse_results(section)
    if not results:
        return raw

    return format_results(results, query)


def search(query: str, num: int = DEFAULT_NUM, lang: str = DEFAULT_LANG) -> None:
    """æ‰§è¡Œæœç´¢å¹¶è¾“å‡ºç»“æœã€‚"""
    print(f"ğŸ” æœç´¢: {query}")
    print(f"   è¯­è¨€: {lang} | ç»“æœæ•°: {num}")
    print("-" * 60)

    url = build_search_url(query, num, lang)
    raw = fetch_url(url)

    if raw.startswith("[é”™è¯¯]"):
        print(raw)
        sys.exit(1)

    result = clean_search_results(raw, query)
    print(result)


def read_page(url: str) -> None:
    """è¯»å–å¹¶è¾“å‡ºæŒ‡å®šç½‘é¡µçš„ Markdown å†…å®¹ã€‚"""
    print(f"ğŸ“„ è¯»å–ç½‘é¡µ: {url}")
    print("-" * 60)

    content = fetch_url(url)

    if content.startswith("[é”™è¯¯]"):
        print(content)
        sys.exit(1)

    print(content)


def main():
    parser = argparse.ArgumentParser(
        description="Web Search â€” é€šç”¨ç½‘ç»œæœç´¢å·¥å…·",
        usage="%(prog)s [-h] {query,read} ...",
    )
    subparsers = parser.add_subparsers(dest="command")

    # read å­å‘½ä»¤
    read_parser = subparsers.add_parser("read", help="è¯»å–æŒ‡å®šç½‘é¡µå†…å®¹")
    read_parser.add_argument("url", help="è¦è¯»å–çš„ç½‘é¡µ URL")

    # å¦‚æœç¬¬ä¸€ä¸ªå‚æ•°ä¸æ˜¯ "read"ï¼Œåˆ™è§†ä¸ºæœç´¢æŸ¥è¯¢
    # è§£æå‚æ•°æ—¶éœ€è¦å¤„ç†è¿™ç§æƒ…å†µ
    if len(sys.argv) < 2:
        parser.print_help()
        sys.exit(1)

    # æ£€æŸ¥æ˜¯å¦æ˜¯ read å­å‘½ä»¤
    if sys.argv[1] == "read":
        args = parser.parse_args()
        read_page(args.url)
    else:
        # æœç´¢æ¨¡å¼ï¼šç¬¬ä¸€ä¸ªä½ç½®å‚æ•°æ˜¯æŸ¥è¯¢è¯
        search_parser = argparse.ArgumentParser(
            description="Web Search â€” é€šç”¨ç½‘ç»œæœç´¢å·¥å…·"
        )
        search_parser.add_argument("query", help="æœç´¢å…³é”®è¯")
        search_parser.add_argument(
            "--num",
            type=int,
            default=DEFAULT_NUM,
            help=f"è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤ {DEFAULT_NUM}ï¼‰",
        )
        search_parser.add_argument(
            "--lang",
            default=DEFAULT_LANG,
            help=f"æœç´¢è¯­è¨€ï¼ˆé»˜è®¤ {DEFAULT_LANG}ï¼‰",
        )
        args = search_parser.parse_args()
        search(args.query, args.num, args.lang)


if __name__ == "__main__":
    main()
